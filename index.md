# Digital-CV
## ABOUT ME
Experienced Software Engineer with expertise in Machine Learning and Back-End Engineering for Full-Stack Data Science projects: Data-Engineering, Server-Side (small to mid-scale web applications), Applied-Statistics/Machine-Learning. Have demonstrated resiliency, drive to innovate, ability to adapt to different technology stacks and leverage them to build robust web and data products. Accustomed to working effectively with highly cross-functional teams. 

## MY EXPERIENCE

- ### Cyncly / Sr Data Engineer (Oct 2023 – June 2024)  

  - #### Computer-Vision/AI Features | Interior Design Software (Python, MongoDB, Azure Synapse, Service Bus, FastAPI/Flask, Data Lake, Vertex AI): 
  
    As the newly formed A.I. Center of Excellence's first data engineer, I got to influence the team's strategic direction for its data operations as well as envision and implement mission-critical data-products and pipelines, with guidance from the team's Senior Architect, and in the process make hiring decisions to expand the team as well.    

    - Collaborated with AI teams to design data-models for a vector database, to optimize read-operations as needed by dependent REST APIs, while simultaneously contributing to the implementation of relevant APIs. 

    - Ideated and implemented an on-demand, idempotent batch data pipeline, hosted on Azure Synapse, empowering downstream teams with a self-serve tool, reducing time of data acquisition and rationalization by 90%. 

    - Implemented logging modules for convenient monitoring of highly complex data-transformation procedures, along with proposing incorporation of event-sourcing pattern into existing workflows for supporting debugging efforts. 


- ### Amazon | Analytics (Business-Intelligence) Engineer ((June 2021 – Oct 2023) | Selected Work 

  - #### Workforce Analytics (AWS-Lambda, Python, QuickSight, Pub-Sub (SNS-SQS), AWS Firehose, Redshift/Athena, PySpark, AWS Glue, AWS Step Functions, AWS S3): 
    Built multiple data-ingestion (real-time) and analytics workflows for customers at Amazon Science. The KPIs generated enabled ML-Engineers to gauge quality of human-annotated images’ data, used for training Computer Vision models, deployed at Amazon’s Robotics Fulfilment centers. 

    - As a critical step in training production grade ML models, I implemented pipelines and workflows, to enable an intricate multi-stage auditing of human annotations on image-data by respective Ops managers, to minimise occurrences of discrepant annotations. Further, I created requisite datasets with important fields generated by the workflows, enabling required insights generation. 

    - Put contingency plans in place to respond to production issues in a time-effective manner. Took initiatives to implement mechanisms to mitigate or fix data-quality problems improving time-to-resolution of bugs by roughly 50%. 

    - Worked on effective partitioning strategies within different data-lake zones and explored query prioritization features using query and user groups on redshift to improve response times.  

    - Developed readable and re-usable code modules, inculcating relevant design principles, and leveraged event-driven architectural patterns to create idempotent and fault-tolerant pipelines with well-planned logging and carefully attached alerts included.  

    - Proposed and ran experiments for an ML use-case to use image-annotations' meta-data to predict the time taken for annotating each image (takt) by a particular associate. Experimented with robust-regression models (handling outliers), and devised/implemented methods to flag takt as anomalous for new production jobs based on error between predicted and actual.
   

  - #### Tool to Review Org Reporting Structure (DynamoDB, AWS-Lambda, Python, GraphQL):
    Developed server-side procedures/code for a small-scale tool, to implement logic leveraging tree-traversal methods to navigate and enforce user made alterations in the org-structure: tree-abstract-data-structure/hierarchical-data stored in JSON format as items in DynamoDb. Considered multiple options and made trade-offs in terms of how to make the updates and persist the updated data in DynamoDb.


- ### Tredence Analytics Solutions Pvt. Ltd. |Software Engineer (June 2018 – July 2020) | Senior Software Engineer (July 2020 – June 2021) | Selected Work 

  - #### Large Scale Machine Learning/Demand forecasting|(ExpressJS, MS-SQL, ORM/Sequelize, PySpark/Spark-SQL, fbprophet, Azure Databricks):

    Worked in a highly cross-functional team to deliver an end-to-end analytics solution which processed large volumes of batch-data to provide insights into product sales of a multinational consumer goods and personal care corporation.
    
      - Co-designed and implemented a demand-forecasting/ML pipeline using PySpark in Azure Databricks to periodically generate constrained and unconstrained demand forecasts.
        
      - Worked with clients to define/implement methods to identify different inventory problems (eg. Out of Stock, Phantom Inventory) and calculate daily historical/future lost sales (~ several millions of dollars) due to each for various products across thousands of stores.
        
      - Implemented and optimized read-intensive RESTful services and data-models using ExpressJS and MS-SQL to enable features like multi-tenancy and embedding of tenant specific power-BI dashboard views in the client-side ReactJS application.
   
      - Monitored API response-times using Apache JMeter under concurrent loads and improved the same to remain within SLA parameters by incorporating simple caching where needed.     

  - #### Digital Customer Experience Platform (Flask/Python/MySQL/Google-Cloud-Platform/OAuth-SSO/Microservices):
    A digital customer experience platform - a suite of web applications (Flask/Django) - to track customer journey and engage with them via different channels at various touchpoints. The efforts were aimed at enhancing customer satisfaction and minimizing churn.
      








